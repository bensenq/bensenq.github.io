{"meta":{"title":"Chris' Blog","subtitle":"learning by making","description":"Chris Tsui's personal blog, mainly about computer tech.","author":"Chris","url":"http://bensenq.github.io"},"pages":[],"posts":[{"title":"IPMI小记","slug":"ipmi-note","date":"2017-11-22T17:40:05.000Z","updated":"2017-11-22T17:45:15.105Z","comments":true,"path":"2017/11/23/ipmi-note/","link":"","permalink":"http://bensenq.github.io/2017/11/23/ipmi-note/","excerpt":"","text":"IPMI简介智能平台管理接口（Intelligent Platform Management Interface）是一个开放的标准，定义了扩平台、跨操作系统监控服务器运行的接口。IPMI卡独立于服务器板主机系统，因此服务器的运行情况不影响监控功能，例如服务器宕机后，可以利用IPMI远程收集信息和重启。在计算机领域这种管理方式通常叫做带外管理（Out-of-band management），特指使用独立管理通道进行设备维护。它允许系统管理员远程监控和管理服务器和其他网络设备，无论这些设备是否处于开机状态。 IPMI只定义架构和接口格式，详细实现各厂商没有统一标准。 IPMI结构IPMI的主控叫BMC（baseboard management controller），其他管理控制器叫做卫星控制器（satellite controllers）。BMC是定制的嵌入式控制器，运行管理系统软件；管理控制器与平台的硬件连接，获取温度、电源、风扇转速、操作系统状态等信心。BMC与周边控制器通过IPMB总线连接，IPMB是I2C的增强实现。BMC也可以通过IPMC总线或桥与其他的BMC及其附属控制器连接。 Side-band vs. out-of-bandIPMI卡在实现上分两种，一种是BMC集成独立NIC，对外提供远程服务，叫做out-of-band方式；另一种方式是BMC通过SMBus连接主机系统的NIC（如上图），与Host OS共享NIC，称作Side-band方式。Side-band的优点是可以节省成本，缺点是带宽受限（因为是SMBus），一般只能支持文本控制台。另外，在Side-band方式下，一旦主机的NIC发生硬件故障，IPMI就没办法通过网络对外提供管理服务了。 关于Side-band的实现细节，即BMC和Host OS共享NIC的技术细节，可以参考这篇文章和这个问答。其核心是NIC的多MAC支持功能，可以把网卡的这种实现方式理解成一个网络交换机：将一个物理NIC和两个虚拟NIC桥接。问答里还列出了Side-band方式的更多缺陷，主要是BMC和HOST没有办法保证真正的隔离。","raw":null,"content":null,"categories":[],"tags":[{"name":"IPMI","slug":"IPMI","permalink":"http://bensenq.github.io/tags/IPMI/"},{"name":"Server","slug":"Server","permalink":"http://bensenq.github.io/tags/Server/"}]},{"title":"Linux驱动模型Probe解惑","slug":"驱动探测解疑","date":"2017-10-31T05:42:33.000Z","updated":"2017-10-31T08:08:21.530Z","comments":true,"path":"2017/10/31/驱动探测解疑/","link":"","permalink":"http://bensenq.github.io/2017/10/31/驱动探测解疑/","excerpt":"","text":"问题首先来回顾下，Linux设备驱动模型中bus、device和driver三者的关系： bus是物理总线的抽象。 device是设备抽象，存在于bus之上。 driver是驱动抽象，注册到bus上，用于驱动bus上的特定device。 device和driver通过bus提供的match方法来匹配（通常是使用设备ID进行匹配）。 driver匹配到device后，调用driver的probe接口驱动device。 一个driver可以驱动多个相同的设备或者不同的设备。 一个driver匹配并驱动多个device的情形比较常见，比如一个igb驱动可以驱动多块intel网卡（可以是相同型号，也可以是不同型号），这是由驱动的id_table和驱动的处理逻辑决定的。那么自然而然的一个问题是：如果系统有多个driver都可以匹配到一个device，系统会怎么处理？换句话说，Linux是否允许两个driver服务同一个device？ 实验带着这样的疑问，我做了下面这个实验。 首先，查看下系统的PCI设备驱动情况，挑选一个测试对象。 12345678910111213141516$ lspci 00:00.0 Host bridge: Intel Corporation Skylake Host Bridge/DRAM Registers (rev 08)00:02.0 VGA compatible controller: Intel Corporation HD Graphics 520 (rev 07)00:14.0 USB controller: Intel Corporation Sunrise Point-LP USB 3.0 xHCI Controller (rev 21)00:14.2 Signal processing controller: Intel Corporation Sunrise Point-LP Thermal subsystem (rev 21)00:16.0 Communication controller: Intel Corporation Sunrise Point-LP CSME HECI #1 (rev 21)00:17.0 SATA controller: Intel Corporation Sunrise Point-LP SATA Controller [AHCI mode] (rev 21)00:1c.0 PCI bridge: Intel Corporation Sunrise Point-LP PCI Express Root Port (rev f1)00:1c.2 PCI bridge: Intel Corporation Sunrise Point-LP PCI Express Root Port (rev f1)00:1f.0 ISA bridge: Intel Corporation Sunrise Point-LP LPC Controller (rev 21)00:1f.2 Memory controller: Intel Corporation Sunrise Point-LP PMC (rev 21)00:1f.3 Audio device: Intel Corporation Sunrise Point-LP HD Audio (rev 21)00:1f.4 SMBus: Intel Corporation Sunrise Point-LP SMBus (rev 21)00:1f.6 Ethernet controller: Intel Corporation Ethernet Connection I219-V (rev 21)02:00.0 Unassigned class [ff00]: Realtek Semiconductor Co., Ltd. RTS522A PCI Express Card Reader (rev 01)04:00.0 Network controller: Intel Corporation Wireless 8260 (rev 3a) 可以看到00:1f.6设备是一块Intel网卡，我们就拿它试验。看看它的驱动情况：12345678$ lspci -v00:1f.6 Ethernet controller: Intel Corporation Ethernet Connection I219-V (rev 21) Subsystem: Lenovo Ethernet Connection I219-V Flags: bus master, fast devsel, latency 0, IRQ 128 Memory at f1200000 (32-bit, non-prefetchable) [size=128K] Capabilities: &lt;access denied&gt; Kernel driver in use: e1000e Kernel modules: e1000e 可以看到当前正在使用的驱动是e1000e模块。 然后，构造一个可以匹配选中网卡的PCI设备驱动，观察驱动加载时系统的行为。 代码直接从ldd3的pci_skel样例修改，只需要把id_table修改成目标网卡的vendor id和device id即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;linux/kernel.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/pci.h&gt;#include &lt;linux/init.h&gt;static struct pci_device_id ids[] = &#123; &#123; PCI_DEVICE(0x8086, 0x1570), &#125;, &#123; 0, &#125;&#125;;MODULE_DEVICE_TABLE(pci, ids);static unsigned char skel_get_revision(struct pci_dev *dev)&#123; u8 revision; pci_read_config_byte(dev, PCI_REVISION_ID, &amp;revision); return revision;&#125;static int probe(struct pci_dev *dev, const struct pci_device_id *id)&#123; /* Do probing type stuff here. * Like calling request_region(); */ printk(\"pci_e1000e probing\\n\"); pci_enable_device(dev); if (skel_get_revision(dev) == 0x42) return -ENODEV; return 0;&#125;static void remove(struct pci_dev *dev)&#123; /* clean up any allocated resources and stuff here. * like call release_region(); */&#125;static struct pci_driver pci_driver = &#123; .name = \"pci_e1000e\", .id_table = ids, .probe = probe, .remove = remove,&#125;;static int __init pci_skel_init(void)&#123; printk(\"%s:%d\\n\", __func__, __LINE__); return pci_register_driver(&amp;pci_driver);&#125;static void __exit pci_skel_exit(void)&#123; pci_unregister_driver(&amp;pci_driver);&#125;MODULE_LICENSE(\"GPL\");module_init(pci_skel_init);module_exit(pci_skel_exit); Makefile如下： 12345678910obj-m := pci_e1000e.oKERNELDIR ?= /lib/modules/$(shell uname -r)/buildPWD := $(shell pwd)all: $(MAKE) -C $(KERNELDIR) M=$(PWD)clean: rm -rf *.o *~ core .depend .*.cmd *.ko *.mod.c .tmp_versions 我在init和probe接口中增加了打印，跟踪流程。 直接make生成pci_e1000e.ko模块，加载测试。 12$ make$ sudo insmod pci_e1000e.ko dmesg查看驱动加载情况，发现内核只新增了一行打印： 1pci_skel_init:52 说明驱动成功加载，但是压根没有进入probe接口。由此看来，内核应该不会让一个device同时被两个driver匹配上。 分析直接从源码分析，可以看到设备驱动加载后，pci_register_driver调用流程大致如下： 其核心代码是driver_attatch，其作用是driver binding，它调用bus_for_each_dev来遍历总线上的所有设备，然后对每一个设备调用__driver_attach函数。 1234int driver_attach(struct device_driver *drv)&#123; return bus_for_each_dev(drv-&gt;bus, NULL, drv, __driver_attach);&#125; 我们想要的答案就在__driver_attach函数的实现里： 1234567891011121314151617181920212223242526272829static int __driver_attach(struct device *dev, void *data)&#123; struct device_driver *drv = data; int ret; ret = driver_match_device(drv, dev); if (ret == 0) &#123; /* no match */ return 0; &#125; else if (ret == -EPROBE_DEFER) &#123; dev_dbg(dev, \"Device match requests probe deferral\\n\"); driver_deferred_probe_add(dev); &#125; else if (ret &lt; 0) &#123; dev_dbg(dev, \"Bus failed to match device: %d\", ret); return ret; &#125; /* ret &gt; 0 means positive match */ if (dev-&gt;parent) /* Needed for USB */ device_lock(dev-&gt;parent); device_lock(dev); if (!dev-&gt;driver) driver_probe_device(drv, dev); device_unlock(dev); if (dev-&gt;parent) device_unlock(dev-&gt;parent); return 0;&#125; 可以看到，在driver_match_device匹配成功以后，并不是直接进行probe的，而是先要判断dev-&gt;driver是否为空，如果不为空就不进行driver_probe_device了。也就是说，如果一个device已经绑定了一个driver，就不允许尝试绑定第二个driver了。","raw":null,"content":null,"categories":[],"tags":[{"name":"Driver","slug":"Driver","permalink":"http://bensenq.github.io/tags/Driver/"}]},{"title":"IRQ Balance","slug":"irq-balance","date":"2017-02-05T11:36:47.000Z","updated":"2017-10-13T06:42:29.644Z","comments":true,"path":"2017/02/05/irq-balance/","link":"","permalink":"http://bensenq.github.io/2017/02/05/irq-balance/","excerpt":"","text":"关于Linux的中断的balance，这篇文章写的很好，可以学习一下。看完后我的理解是这样的: 多核情况下，为了发挥并发优势，希望中断能分发到不同的核上并发处理，但是需要注意，数据在不同CPU的Cache上来回的迁移是很耗性能的，因此Linux的默认行为也是将某一中断绑定到一个核，而不是在多个核上轮播。 但是一般中断处理代码都比较短，Cache的数据量不大，因此如果能自动在多个CPU上轮播中断是有可能提升性能的。但是问题又来了，网卡的收发包过程中有TCP连接状态的缓存，是非常适合Cache进行缓存的，因此在这种情况下轮播中断变得不可取，因为会导致严重的Cache数据颠簸，性能大大下降。 为了解决这个问题，现代的网卡都是采用PCI MSIX中断，也就是单设备多中断，可以为每个硬件收发队列分配一个中断，然后把每个中断绑定到不同的核上，这样基本上就解决了上面的问题，即能多核并发，又能兼顾收发包的局部性缓存带来的性能收益（网卡驱动会根据IP地址的hash放到对应的队列。） 最后一点，服务器设备比较多，有可能发生高速设备的中断数大于CPU核数，那么就会有一个核处理2个以上中断的情况发上，而这个基本上是没办法提前预知的（和CPU架构、核数、设备拓扑等相关），所以只能通过/proc/irq接口来手动调节。有个用户程序叫irqbalance可以自动做中断均衡，而且据说新版本评价不错，我自己没试过，但我认为要想获取最佳性能还得手动设置affinity。","raw":null,"content":null,"categories":[],"tags":[{"name":"IRQ","slug":"IRQ","permalink":"http://bensenq.github.io/tags/IRQ/"},{"name":"Perf","slug":"Perf","permalink":"http://bensenq.github.io/tags/Perf/"}]},{"title":"Docker初试","slug":"Docker初试","date":"2017-01-01T16:47:29.000Z","updated":"2017-10-13T06:45:30.165Z","comments":true,"path":"2017/01/02/Docker初试/","link":"","permalink":"http://bensenq.github.io/2017/01/02/Docker初试/","excerpt":"","text":"要了解一样东西，最简单的入门方法无非是去使用它，跟它交互，一步步填补概念和纠正已有认知。从今天开始踏上Docker之路！ 安装ubuntu 16.04上的安装还是比较方便的，根据官方文档，添加APT源，然后直接apt-get install docker-engine就ok了。具体命令如下：123456789$ sudo apt-get update$ sudo apt-get install apt-transport-https ca-certificates$ sudo apt-key adv \\ --keyserver hkp://ha.pool.sks-keyservers.net:80 \\ --recv-keys 58118E89F3A912897C070ADBF76221572C52609D$ echo \"deb https://apt.dockerproject.org/repo ubuntu-xenial main\" | sudo tee /etc/apt/sources.list.d/docker.list$ sudo apt-get update$ sudo apt-get install linux-image-extra-$(uname -r) linux-image-extra-virtual$ sudo apt-get install docker-engine 小试开启docker服务docker daemon负责接收docker工具发过来的命令，然后做相应的操作，因此首先要开启它：1$ sudo service docker start hello worlddocker提供了一个最基本的hello world镜像，用来验证docker安装是否就绪。利用docker命令运行它：1$ sudo docker run hello-world 如果正常一切正常，应该有如下输出：12345678910111213141516171819Hello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker Hub account: https://hub.docker.comFor more examples and ideas, visit: https://docs.docker.com/engine/userguide/ 可以看到，docker为了运行hello world例程，经过了一下几个步骤： Docker客户端（也就是Docker命令）连接Docker Daemon Docker Daemon从Docker Hub（也就是Docker应用仓库）中下载hello-world镜像 Docker Daemon根据hello-world镜像建立一个Container实例，该实例会运行可执行文件并输出上述打印 Docker Daemon把输出定向给Docker客户端，显示在终端 深入一点hello-world生命周期很短，输出消息后就退出了，看不到太多东东。根据上面的提示，我们可以在容器里运行个ubuntu镜像玩玩：1$ docker run -it ubuntu bash 其中-it是用来生成交互的伪终端与Container交互，ubuntu是镜像名。和hello-world例子不一样，这个例子多了参数bash，这是用来指明要在Docker中运行的进程，因此上面命令的意思是说，给我创建一个ubuntu容器，并运行bash shell。 如果不加结尾的bash，我们发现效果与bash相同，也就是说缺省情况下ubuntu镜像就会启动一个bash命令行. 把bash改成我们想要的命令，就可以在容器中运行它。换成ls试试：123 $ docker run -it ubuntu lsbin dev home lib64 mnt proc run srv tmp varboot etc lib media opt root sbin sys usr 还可以给命令加参数：123456789101112131415161718192021$ sudo docker run -it ubuntu ls -lrttotal 64drwxr-xr-x 8 root root 4096 Sep 13 2015 libdrwxr-xr-x 2 root root 4096 Apr 12 2016 homedrwxr-xr-x 2 root root 4096 Apr 12 2016 bootdrwxr-xr-x 2 root root 4096 Dec 13 20:23 srvdrwxr-xr-x 2 root root 4096 Dec 13 20:23 optdrwxr-xr-x 2 root root 4096 Dec 13 20:23 mntdrwxr-xr-x 2 root root 4096 Dec 13 20:23 mediadrwxr-xr-x 2 root root 4096 Dec 13 20:24 lib64drwx------ 2 root root 4096 Dec 13 20:24 rootdrwxr-xr-x 2 root root 4096 Dec 13 20:24 bindrwxrwxrwt 2 root root 4096 Dec 13 20:24 tmpdrwxr-xr-x 11 root root 4096 Dec 15 17:45 usrdrwxr-xr-x 2 root root 4096 Dec 15 17:45 sbindrwxr-xr-x 13 root root 4096 Dec 15 17:45 vardrwxr-xr-x 6 root root 4096 Dec 15 17:45 rundr-xr-xr-x 13 root root 0 Dec 28 17:01 sysdrwxr-xr-x 45 root root 4096 Dec 28 17:35 etcdr-xr-xr-x 221 root root 0 Dec 28 17:35 procdrwxr-xr-x 5 root root 380 Dec 28 17:35 dev docker的容器本质上是提供了一个与主机环境隔离的执行环境，普通Linux上的1号进程是特殊的init，而容器环境下的1号进程就是启动容器时指定的应用。譬如，用ps命令试试看：123$ sudo docker run ubuntu:latest ps PID TTY TIME CMD 1 ? 00:00:00 ps 可以看到，ps命令在容器里的PID就是1。当然除了rootfs不同、进程PID编号不同，容器还在进程间通信、用户、文件系统挂载点、主机名称及物理资源（CPU、MEM、网络）等方面进行隔离和限制，后面逐步探索。","raw":null,"content":null,"categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://bensenq.github.io/tags/Docker/"}]},{"title":"readl和raw_readl语义区别","slug":"raw_readl","date":"2016-08-27T09:25:42.000Z","updated":"2017-10-13T06:51:52.241Z","comments":true,"path":"2016/08/27/raw_readl/","link":"","permalink":"http://bensenq.github.io/2016/08/27/raw_readl/","excerpt":"","text":"进行Linux驱动方面开发的程序员，经常需要使用readl/writel系列函数对Memory-Mapped IO进行读写。那么readl/__readl/raw_readl/__raw_readl几个函数的具体语义有何差别呢？ 内核中关于这几个函数的说明非常有限，根据stackoverflow上的这个问答，我们可以总结为： raw前缀只与byteorder相关，即readl/writel是linux默认的小端操作，而raw_readl/raw_writel是native访问。也就是说：如果是小端系统，raw_readl与readl相同，如果是大端系统，raw_readl与readl的有字节序差别。 双下划线前缀与指令保序相关，即readl/writel包含存储器栅栏指令mb，能够保证IO读写顺序，而__readl/__writel则不能保证。 另外，有的体系结构还会定义readl_relaxed/writel_relaxed接口，其含义应该与__readl/__writel相同，表示小端、不带存储器栅栏的读写。","raw":null,"content":null,"categories":[],"tags":[{"name":"Kernel","slug":"Kernel","permalink":"http://bensenq.github.io/tags/Kernel/"}]},{"title":"GNU三元操作符扩展","slug":"GNU三元操作符扩展","date":"2016-03-03T14:12:29.000Z","updated":"2017-10-13T06:49:40.049Z","comments":true,"path":"2016/03/03/GNU三元操作符扩展/","link":"","permalink":"http://bensenq.github.io/2016/03/03/GNU三元操作符扩展/","excerpt":"","text":"Linux内核随处可见到关于三元操作符？:的这种使用方法： 1rdev-&gt;map_name = map_name ?: RC_MAP_EMPTY; 这是GNU C的关于标准C三元操作符?:的一个扩展，用来省略条件表达式的中间参数。也就是说，x ? : y 与 x ？ x : y完全等价。 所有GNU扩展都在GCC手册的Extensions to the C Language Family章节有详细描述。要注意，这些写法仅适合GCC编译器，如果要编写可移植的代码，那么就要格外小心。","raw":null,"content":null,"categories":[],"tags":[{"name":"C","slug":"C","permalink":"http://bensenq.github.io/tags/C/"},{"name":"GNU","slug":"GNU","permalink":"http://bensenq.github.io/tags/GNU/"}]}]}