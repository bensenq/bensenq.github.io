<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>操作系统对NUMA支持情况：NUMA基础 &middot; sukihiro</title>
        <meta name="description" content="1. NUMA简介 NUMA表示Non-Uniform Memory Access，非均匀内存访问模型，通常是指内存到每个CPU的&quot;距离&quot;并不相同，这里&quot;距离&quot;同时包括延迟和带宽。在NUMA系统中，这种非均匀性还会扩展到IO资源（主要是PCIE设备），即CPU与IO资源的&quot;距离&quot;以及设备与内存的&quot;距离&quot;也是不同的。一般来说NUMA机器可以访问系统的所有资源（即统一编址特性），只是访问速率不同而已。
NUMA中使用Node表示一组资源（例如CPUs、Memory、I/O)。在某些系统中一个Node可能只包含某几类资源而不是全部，有些系统中一个Node就包括所以这些种类。Node间的互联有不同的形式，但是一般来说Node之间的延迟会比Node内大，而带宽通常会比Node内低。
NUMA上的软件聚焦在Locality上，即尽可能地使用离设备（这里CPU也是算设备）近的资源，减少Node间的数据流量交互。对于Node间有高速互联的系统，把对资源（内存和IO）的使用分散到不同的节点上，而不是仅仅使用本地资源，有时反而能够取得更好的系统带宽（原因是能够存储器控制器的利用率更高），但是这种方法并不普适。
2. NUMA Hierarchy NUMA Hierarchy就是NUMA的层级结构。一个Intel x86 NUMA系统就是由多个NUMA Node组成。
2.1 NUMA Node内部 一个NUMA Node内部是由一个物理CPU和它所有的本地内存(Local Memory)组成的。广义得讲， 一个NUMA Node内部还包含本地IO资源，对大多数Intel x86 NUMA平台来说，主要是PCIe总线资源。 ACPI规范就是这么抽象一个NUMA Node的。
物理CPU 一个CPU Socket里可以由多个CPU Core和一个Uncore部分组成。每个CPU Core内部又可以由两个CPU Thread组成。每个CPU thread都是一个操作系统可见的逻辑CPU。对大多数操作系统来说，一个八核HT打开的CPU会被识别为16个CPU。
  Socket
一个Socket对应一个物理CPU。这个词大概是从CPU在主板上的物理连接方式上来的。处理器通过主板的Socket来插到主板上。尤其是有了多核(Multi-core)系统以后，Multi-socket系统被用来指明系统到底存在多少个物理CPU。
  Core
CPU的运算核心。 x86的核包含了CPU运算的基本部件，如逻辑运算单元(ALU), 浮点运算单元(FPU), L1和L2缓存。一个Socket里可以有多个Core。如今的多核时代，即使是Single Socket的系统，也是逻辑上的SMP系统。但是，一个物理CPU的系统不存在非本地内存，因此相当于UMA系统。
  Uncore
Intel x86物理CPU里没有放在Core里的部件都被叫做Uncore。Uncore里集成了过去x86 UMA架构时代北桥芯片的基本功能。在Nehalem时代，内存控制器被集成到CPU里，叫做iMC(Integrated Memory Controller)。而PCIe Root Complex还做为独立部件在IO Hub芯片里。到了SandyBridge时代，PCIe Root Complex也被集成到了CPU里。现今的Uncore部分，除了iMC，PCIe Root Complex，还有QPI(QuickPath Interconnect)控制器，L3缓存，CBox(负责缓存一致性)，及其它外设控制器。
  Threads
这里特指CPU的多线程技术。在Intel x86架构下，CPU的多线程技术被称作超线程(Hyper-Threading)技术。Intel的超线程技术在一个处理器Core内部引入了额外的硬件设计模拟了两个逻辑处理器(Logical Processor)，每个逻辑处理器都有独立的处理器状态，但共享Core内部的计算资源，如ALU，FPU，L1，L2缓存。这样在最小的硬件投入下提高了CPU在多线程软件工作负载下的性能，提高了硬件使用效率。x86的超线程技术出现早于NUMA架构。
  本地内存 在Intel x86平台上，所谓本地内存，就是CPU可以经过Uncore部件里的iMC访问到的内存。而那些非本地的，远程内存(Remote Memory)，则需要经过QPI的链路到该内存所在的本地CPU的iMC来访问。曾经在Intel IvyBridge的NUMA平台上做的内存访问性能测试显示，远程内存访问的延时时本地内存的一倍。可以假设，操作系统应该尽量利用本地内存的低访问延迟特性来优化应用和系统的性能。">
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="generator" content="Hugo 0.80.0" />
        <meta name="robots" content="index,follow">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta property="og:title" content="操作系统对NUMA支持情况：NUMA基础">
<meta property="og:description" content="1. NUMA简介 NUMA表示Non-Uniform Memory Access，非均匀内存访问模型，通常是指内存到每个CPU的&quot;距离&quot;并不相同，这里&quot;距离&quot;同时包括延迟和带宽。在NUMA系统中，这种非均匀性还会扩展到IO资源（主要是PCIE设备），即CPU与IO资源的&quot;距离&quot;以及设备与内存的&quot;距离&quot;也是不同的。一般来说NUMA机器可以访问系统的所有资源（即统一编址特性），只是访问速率不同而已。
NUMA中使用Node表示一组资源（例如CPUs、Memory、I/O)。在某些系统中一个Node可能只包含某几类资源而不是全部，有些系统中一个Node就包括所以这些种类。Node间的互联有不同的形式，但是一般来说Node之间的延迟会比Node内大，而带宽通常会比Node内低。
NUMA上的软件聚焦在Locality上，即尽可能地使用离设备（这里CPU也是算设备）近的资源，减少Node间的数据流量交互。对于Node间有高速互联的系统，把对资源（内存和IO）的使用分散到不同的节点上，而不是仅仅使用本地资源，有时反而能够取得更好的系统带宽（原因是能够存储器控制器的利用率更高），但是这种方法并不普适。
2. NUMA Hierarchy NUMA Hierarchy就是NUMA的层级结构。一个Intel x86 NUMA系统就是由多个NUMA Node组成。
2.1 NUMA Node内部 一个NUMA Node内部是由一个物理CPU和它所有的本地内存(Local Memory)组成的。广义得讲， 一个NUMA Node内部还包含本地IO资源，对大多数Intel x86 NUMA平台来说，主要是PCIe总线资源。 ACPI规范就是这么抽象一个NUMA Node的。
物理CPU 一个CPU Socket里可以由多个CPU Core和一个Uncore部分组成。每个CPU Core内部又可以由两个CPU Thread组成。每个CPU thread都是一个操作系统可见的逻辑CPU。对大多数操作系统来说，一个八核HT打开的CPU会被识别为16个CPU。
  Socket
一个Socket对应一个物理CPU。这个词大概是从CPU在主板上的物理连接方式上来的。处理器通过主板的Socket来插到主板上。尤其是有了多核(Multi-core)系统以后，Multi-socket系统被用来指明系统到底存在多少个物理CPU。
  Core
CPU的运算核心。 x86的核包含了CPU运算的基本部件，如逻辑运算单元(ALU), 浮点运算单元(FPU), L1和L2缓存。一个Socket里可以有多个Core。如今的多核时代，即使是Single Socket的系统，也是逻辑上的SMP系统。但是，一个物理CPU的系统不存在非本地内存，因此相当于UMA系统。
  Uncore
Intel x86物理CPU里没有放在Core里的部件都被叫做Uncore。Uncore里集成了过去x86 UMA架构时代北桥芯片的基本功能。在Nehalem时代，内存控制器被集成到CPU里，叫做iMC(Integrated Memory Controller)。而PCIe Root Complex还做为独立部件在IO Hub芯片里。到了SandyBridge时代，PCIe Root Complex也被集成到了CPU里。现今的Uncore部分，除了iMC，PCIe Root Complex，还有QPI(QuickPath Interconnect)控制器，L3缓存，CBox(负责缓存一致性)，及其它外设控制器。
  Threads
这里特指CPU的多线程技术。在Intel x86架构下，CPU的多线程技术被称作超线程(Hyper-Threading)技术。Intel的超线程技术在一个处理器Core内部引入了额外的硬件设计模拟了两个逻辑处理器(Logical Processor)，每个逻辑处理器都有独立的处理器状态，但共享Core内部的计算资源，如ALU，FPU，L1，L2缓存。这样在最小的硬件投入下提高了CPU在多线程软件工作负载下的性能，提高了硬件使用效率。x86的超线程技术出现早于NUMA架构。
  本地内存 在Intel x86平台上，所谓本地内存，就是CPU可以经过Uncore部件里的iMC访问到的内存。而那些非本地的，远程内存(Remote Memory)，则需要经过QPI的链路到该内存所在的本地CPU的iMC来访问。曾经在Intel IvyBridge的NUMA平台上做的内存访问性能测试显示，远程内存访问的延时时本地内存的一倍。可以假设，操作系统应该尽量利用本地内存的低访问延迟特性来优化应用和系统的性能。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://www.sukihiro.cn/2020/03/25/numa-os-part1/">
        <link rel="stylesheet" href="https://www.sukihiro.cn/dist/site.css">
        <link rel="stylesheet" href="https://www.sukihiro.cn/dist/syntax.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,400,600,700,300&subset=latin,cyrillic-ext,latin-ext,cyrillic">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
        
        
        
        
        

    </head>
    <body>
        

        <div id="wrapper">
            <header class="site-header">
                <div class="container">
                    <div class="site-title-wrapper">
                        
                            <h1 class="site-title">
                                <a href="https://www.sukihiro.cn/">Chris&#39; Blog</a>
                            </h1>
                        
                        
                            <a class="button-square" href="https://www.sukihiro.cn/index.xml" aria-label="RSS"><i class="fa fa-rss" aria-hidden="true"></i></a>
                        
                        
                        
                        
                        
                            <a class="button-square button-social hint--top" data-hint="Github" aria-label="Github" href="https://github.com/bensenq" rel="me">
                                <i class="fa fa-github-alt" aria-hidden="true"></i>
                            </a>
                        
                        
                        
                        
                            <a class="button-square button-social hint--top" data-hint="Email" aria-label="Send an Email" href="mailto:chris.wei.cui@gmail.com">
                                <i class="fa fa-envelope" aria-hidden="true"></i>
                            </a>
                        
                    </div>

                    <ul class="site-nav">
                        
    <li class="site-nav-item">
        <a href="/">Blog</a>
    </li>

    <li class="site-nav-item">
        <a href="/about/">About</a>
    </li>


                    </ul>
                </div>
            </header>

            <div id="container">


<div class="container">
    <article class="post-container" itemscope="" itemtype="http://schema.org/BlogPosting">
        <header class="post-header">
    <h1 class="post-title" itemprop="name headline">操作系统对NUMA支持情况：NUMA基础</h1>
    
    <p class="post-date">
        <span>Published <time datetime="2020-03-25" itemprop="datePublished">Wed, Mar 25, 2020</time></span>
        <span>by</span>
        <span itemscope="" itemprop="author" itemtype="https://schema.org/Person">
            <span itemprop="name">
                <a href="#" itemprop="url" rel="author">sukihiro</a>
            </span>
        </span>
    </p>
    
        <p class="post-reading post-line">
            <span>Estimated reading time: 1 min</span>
        </p>
    
</header>

        <div class="post-content clearfix" itemprop="articleBody">
    

    <h2 id="1-numa简介">1. NUMA简介</h2>
<p>NUMA表示Non-Uniform Memory Access，非均匀内存访问模型，通常是指内存到每个CPU的&quot;距离&quot;并不相同，这里&quot;距离&quot;同时包括延迟和带宽。在NUMA系统中，这种非均匀性还会扩展到IO资源（主要是PCIE设备），即CPU与IO资源的&quot;距离&quot;以及设备与内存的&quot;距离&quot;也是不同的。一般来说NUMA机器可以访问系统的所有资源（即统一编址特性），只是访问速率不同而已。</p>
<p>NUMA中使用Node表示一组资源（例如CPUs、Memory、I/O)。在某些系统中一个Node可能只包含某几类资源而不是全部，有些系统中一个Node就包括所以这些种类。Node间的互联有不同的形式，但是一般来说Node之间的延迟会比Node内大，而带宽通常会比Node内低。</p>
<p><img src="/images/NUMA.png" alt="NUMA"></p>
<p>NUMA上的软件聚焦在Locality上，即尽可能地使用离设备（这里CPU也是算设备）近的资源，减少Node间的数据流量交互。对于Node间有高速互联的系统，把对资源（内存和IO）的使用分散到不同的节点上，而不是仅仅使用本地资源，有时反而能够取得更好的系统带宽（原因是能够存储器控制器的利用率更高），但是这种方法并不普适。</p>
<h2 id="2-numa-hierarchy">2. NUMA Hierarchy</h2>
<p>NUMA Hierarchy就是NUMA的层级结构。一个Intel x86 NUMA系统就是由多个NUMA Node组成。</p>
<h3 id="21-numa-node内部">2.1 NUMA Node内部</h3>
<p>一个NUMA Node内部是由一个<strong>物理CPU</strong>和它所有的<strong>本地内存(Local Memory)<strong>组成的。广义得讲，
一个NUMA Node内部还包含</strong>本地IO资源</strong>，对大多数Intel x86 NUMA平台来说，主要是PCIe总线资源。
ACPI规范就是这么抽象一个NUMA Node的。</p>
<h4 id="物理cpu">物理CPU</h4>
<p>一个CPU Socket里可以由多个CPU Core和一个Uncore部分组成。每个CPU Core内部又可以由两个CPU Thread组成。每个CPU thread都是一个操作系统可见的逻辑CPU。对大多数操作系统来说，一个八核HT打开的CPU会被识别为16个CPU。</p>
<p><img src="/images/NUMA_Hierachy.png" alt="NUMA_Hierachy"></p>
<ul>
<li>
<p>Socket</p>
<p>一个Socket对应一个物理CPU。这个词大概是从CPU在主板上的物理连接方式上来的。处理器通过主板的Socket来插到主板上。尤其是有了多核(Multi-core)系统以后，Multi-socket系统被用来指明系统到底存在多少个物理CPU。</p>
</li>
<li>
<p>Core</p>
<p>CPU的运算核心。 x86的核包含了CPU运算的基本部件，如逻辑运算单元(ALU), 浮点运算单元(FPU), L1和L2缓存。一个Socket里可以有多个Core。如今的多核时代，即使是Single Socket的系统，也是逻辑上的SMP系统。但是，一个物理CPU的系统不存在非本地内存，因此相当于UMA系统。</p>
</li>
<li>
<p>Uncore</p>
<p>Intel x86物理CPU里没有放在Core里的部件都被叫做Uncore。Uncore里集成了过去x86 UMA架构时代北桥芯片的基本功能。在Nehalem时代，内存控制器被集成到CPU里，叫做iMC(Integrated Memory Controller)。而PCIe Root Complex还做为独立部件在IO Hub芯片里。到了SandyBridge时代，PCIe Root Complex也被集成到了CPU里。现今的Uncore部分，除了iMC，PCIe Root Complex，还有QPI(QuickPath Interconnect)控制器，L3缓存，CBox(负责缓存一致性)，及其它外设控制器。</p>
</li>
<li>
<p>Threads</p>
<p>这里特指CPU的多线程技术。在Intel x86架构下，CPU的多线程技术被称作超线程(Hyper-Threading)技术。Intel的超线程技术在一个处理器Core内部引入了额外的硬件设计模拟了两个逻辑处理器(Logical Processor)，每个逻辑处理器都有独立的处理器状态，但共享Core内部的计算资源，如ALU，FPU，L1，L2缓存。这样在最小的硬件投入下提高了CPU在多线程软件工作负载下的性能，提高了硬件使用效率。x86的超线程技术出现早于NUMA架构。</p>
</li>
</ul>
<h4 id="本地内存">本地内存</h4>
<p>在Intel x86平台上，所谓本地内存，就是CPU可以经过Uncore部件里的iMC访问到的内存。而那些非本地的，远程内存(Remote Memory)，则需要经过QPI的链路到该内存所在的本地CPU的iMC来访问。曾经在Intel IvyBridge的NUMA平台上做的内存访问性能测试显示，远程内存访问的延时时本地内存的一倍。可以假设，操作系统应该尽量利用本地内存的低访问延迟特性来优化应用和系统的性能。</p>
<h4 id="本地io资源">本地IO资源</h4>
<p>如前所述，Intel自从SandyBridge处理器开始，已经把PCIe Root Complex集成到CPU里了。正因为如此，从CPU直接引出PCIe Root Port的PCIe 3.0的链路可以直接与PCIe Switch或者PCIe Endpoint相连。一个PCIe Endpoint就是一个PCIe外设。这就意味着，对某个PCIe外设来说，如果它直接于哪个CPU相连，它就属于哪个CPU所在的NUMA Node。</p>
<p>与本地内存一样，所谓本地IO资源，就是CPU可以经过Uncore部件里的PCIe Root Complex直接访问到的IO资源。如果是非本地IO资源，则需要经过QPI链路到该IO资源所属的CPU，再通过该CPU PCIe Root Complex访问。如果同一个NUMA Node内的CPU和内存和另外一个NUMA Node的IO资源发生互操作，因为要跨越QPI链路，会存在额外的访问延迟问题。</p>
<p>其它体系结构里，为降低外设访问延迟，也有将IB(Infiniband)总线集成到CPU里的。这样IB设备也属于NUMA Node的一部分了。</p>
<p>可以假设，操作系统如果是NUMA Aware的话，应该会尽量针对本地IO资源低延迟的优点进行优化。</p>
<h3 id="22-numa-node互联">2.2 NUMA Node互联</h3>
<p>在Intel x86上，NUMA Node之间的互联是通过QPI((QuickPath Interconnect) Link的。CPU的Uncore部分有QPI的控制器来控制CPU到QPI的数据访问。</p>
<p>不借助第三方的Node Controller，2/4/8个NUMA Node(取决于具体架构)可以通过QPI(QuickPath Interconnect)总线互联起来，构成一个NUMA系统。例如，SGI UV计算机系统，它就是借助自家的SGI NUMAlink®互联技术来达到4到256个CPU socket扩展的能力的。这是一个SMP系统，所以支持运行一个Linux操作系统实例去管理系统。</p>
<h2 id="3-numa-affinity">3. NUMA Affinity</h2>
<p>NUMA Affinity(亲和性)是和NUMA Hierarchy(层级结构)直接相关的。对系统软件来说，以下两个概念至关重要:</p>
<ol>
<li>
<p><strong>CPU NUMA Affinity</strong></p>
<p>CPU NUMA的亲和性是指从CPU角度看，哪些内存访问更快，有更低的延迟。如前所述，和该CPU直接相连的本地内存是更快的。操作系统如果可以根据任务所在CPU去分配本地内存，就是基于CPU NUMA亲和性的考虑。因此，CPU NUMA亲和性就是要尽量让任务运行在本地的NUMA Node里。</p>
</li>
<li>
<p><strong>Device NUMA Affinity</strong></p>
<p>设备NUMA亲和性是指从PCIe外设的角度看，如果和CPU和内存相关的IO活动都发生在外设所属的NUMA Node，将会有更低延迟。这里有两种设备NUMA亲和性的问题，</p>
<ul>
<li>
<p><strong>DMA Buffer NUMA Affinity</strong></p>
<p>大部分PCIe设备支持DMA功能的。也就是说，设备可以直接把数据写入到位于内存中的DMA缓冲区。显然，如果DMA缓冲区在PCIe外设所属的NUMA Node里分配，那么将会有最低的延迟。否则，外设的DMA操作要跨越QPI链接去读写另外一个NUMA Node里的DMA缓冲区。因此，操作系统如果可以根据PCIe设备所属的NUMA node分配DMA缓冲区，将会有最好的DMA操作的性能。</p>
</li>
<li>
<p><strong>Interrupt NUMA Affinity</strong></p>
<p>设备DMA操作完成后，需要在CPU上触发中断来通知驱动程序的中断处理例程(ISR)来读写DMA缓冲区。很多时候，ISR触发下半部机制(SoftIRQ)来进入到协议栈相关(Network，Storage)的代码路径来传送数据。对大部分操作系统来说，硬件中断(HardIRQ)和下半部机制的代码在同一个CPU上发生。因此，DMA缓冲区的读写操作发生的位置和设备硬件中断(HardIRQ)密切相关。假设操作系统可以把设备的硬件中断绑定到自己所属的NUMA node，那之后中断处理函数和协议栈代码对DMA缓冲区的读写将会有更低的延迟。</p>
</li>
</ul>
</li>
</ol>
<h2 id="4-numa拓扑信息">4. NUMA拓扑信息</h2>
<p>由于NUMA的亲和性对应用的性能非常重要，那么硬件平台就需要给操作系统提供接口机制来感知硬件的NUMA层级结构。在x86平台，ACPI规范提供了以下接口来让操作系统来检测系统的NUMA层级结构。</p>
<p>ACPI 5.0a规范的第17章是有关NUMA的章节。ACPI规范里，NUMA Node被第9章定义的Module Device所描述。ACPI规范里用<strong>Proximity Domain</strong>对NUMA Node做了抽象，两者的概念大多时候等同。</p>
<ul>
<li>
<p><strong>SRAT(System Resource Affinity Table)</strong></p>
<p>主要描述了系统boot时的CPU和内存都属于哪个Proximity Domain(NUMA Node)。 这个表格里的信息是静态的，如果是启动后热插拔，需要用OSPM的_PXM方法去获得相关信息。</p>
</li>
<li>
<p><strong>SLIT(System Locality Information Table)</strong></p>
<p>提供CPU和内存之间的位置远近信息。在SRAT表格里，只能告诉给定的CPU和内存是否在一个NUMA Node。对某个CPU来说，不在本NUMA Node里的内存，即远程内存们是否都是一样的访问延迟取决于NUMA的拓扑有多复杂(QPI的跳数)。总之，对于不能简单用<strong>远近</strong>来描述的NUMA系统(QPI存在0，1，2等不同跳数)，需要SLIT表格给出进一步的说明。同样的，这个表格也是静态表格，热插拔需要使用OSPM的_SLI方法。</p>
<p><img src="/images/NUMA_Distance.png" alt="NUMA_Distance"></p>
</li>
<li>
<p><strong>DSDT(Differentiated System Description Table)</strong></p>
<p>从Device NUMA角度看，这个表格给出了系统boot时的外设都属于哪个Proximity Domain(NUMA Node)。</p>
</li>
</ul>
<p>ACPI规范OSPM(Operating System-directed configuration and Power Management)和OSPM各种方法就是操作系统里的ACPI驱动和ACPI firmware之间的一个互动的接口。x86启动OS后，没有ACPI之前，firmware(BIOS)的代码是无法被执行了，除非通过SMI中断处理程序。但有了ACPI，BIOS提前把ACPI的一些静态表格和AML的bytecode代码装载到内存，然后ACPI驱动就会加载AML的解释器，这样OS就可以通过ACPI驱动调用预先装载的AML代码。AML(ACPI Machine Language)是和Java类似的一种虚拟机解释型语言，所以不同操作系统的ACPI驱动，只要有相同的虚拟机解释器，就可以直接从操作系统调用ACPI写好的AML的代码了。所以，前文所述的所有热插拔的OSPM方法，其实就是对应ACPI firmware的AML的一段函数代码而已。</p>

</div>

        <footer class="post-footer clearfix">
        <p class="post-tags">
            <span>Tagged:</span>
                <a href="/tags/os/">OS</a>, 
                <a href="/tags/architecture/">Architecture</a>
        </p>
    <div class="share">
    </div>
</footer>


        
    <div class="comments">
        
    </div>

    </article>
</div>

            </div>
        </div>

        <footer class="footer">
            <div class="container">
                <div class="site-title-wrapper">
                    <h1 class="site-title">
                        <a href="https://www.sukihiro.cn/">Chris&#39; Blog</a>
                    </h1>
                    <a class="button-square button-jump-top js-jump-top" href="#" aria-label="Back to Top">
                        <i class="fa fa-angle-up" aria-hidden="true"></i>
                    </a>
                </div>

                <p class="footer-copyright">
                    <span>&copy; 2020 / Powered by <a href="https://gohugo.io/">Hugo</a></span>
                </p>
                <p class="footer-copyright">
                    <span><a href="https://github.com/roryg/ghostwriter">Ghostwriter theme</a> By <a href="http://jollygoodthemes.com">JollyGoodThemes</a></span>
                    <span>/ <a href="https://github.com/jbub/ghostwriter">Ported</a> to Hugo By <a href="https://github.com/jbub">jbub</a></span>
                </p>
            </div>
        </footer>

        <script src="https://www.sukihiro.cn/js/jquery-1.11.3.min.js"></script>
        <script src="https://www.sukihiro.cn/js/jquery.fitvids.js"></script>
        <script src="https://www.sukihiro.cn/js/scripts.js"></script>
    </body>
</html>

