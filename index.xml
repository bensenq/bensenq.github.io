<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chris&#39; Blog</title>
    <link>https://sukihiro.cn/</link>
    <description>Recent content on Chris&#39; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>chris</copyright>
    <lastBuildDate>Sun, 20 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://sukihiro.cn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>一个C语言未定义行为的反思</title>
      <link>https://sukihiro.cn/2022/03/20/return-local-addr/</link>
      <pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://sukihiro.cn/2022/03/20/return-local-addr/</guid>
      <description>作为一个C语言15年+的老玩家，今天载了个跟头，记录反思之。
一切源自于一个bug，如下：
#include &amp;lt;stdio.h&amp;gt; int *func() { int p[10]; return &amp;amp;p[2]; } int main() { int * tmp = func(); printf(&amp;#34;%p\n&amp;#34;, tmp); return 0; } 编译的时候，编译器（gcc）报了个警告： warning: function returns address of local variable。没错，func函数返回了一个局部变量的地址，这是很明显的错误：数组p[]是在栈上分配的，从func返回后，局部变量就消亡了，所以在函数外引用一个局部变量时没有意义。
但关键问题是，main函数里能拿到tmp能拿到这个不应该返回的局部变量地址吗？或者说，尽管无意义，但是func是否可以把这个局部变量的地址返回给调用者（caller）吗？
C语言最大的魅力就在于灵活高效，其精髓就在于指针，但指针是把双刃剑。在遇到这个问题前，我认为caller是可以拿到栈上的这个地址的，也是基于C语言给我的这个印象。我想：虽然这样做是无意义或者危险的，但是C语言并不会阻止我们拿到一个局部变量的地址，毕竟那个局部变量曾经确实存在于一个确定的栈地址！而且拿到这个栈地址跟访问或使用这个内存地址并不是一回事儿！
结果出乎我的意料。caller确实拿到了func的返回值（程序没有报错），但是拿到的却不是&amp;amp;p[2]的地址，而是0。(看到这个结果时，脑子里是OMG，Why？)
全网搜索，大家都在讨论这个warning的原因，但是没有人讨论gcc为什么会给caller一个0。看来只能从C语言标准和编译器实现入手了。用gcc和return local variable address进行关键字搜索，总算有了一个明确的方向：这是一个未定义的行为（undefined behavior）。也就是说C语言没有对这种行为的后果做任何约束，编译器想怎么实现都可以！而gcc遇到这种情况就会给caller返回一个0。
我们来看看以上例子的反汇编：
struct possibleSet *func() {1149: f3 0f 1e fa endbr64114d: 55 push %rbp114e: 48 89 e5 mov %rsp,%rbp1151: 48 81 ec b0 00 00 00 sub $0xb0,%rsp1158: 64 48 8b 04 25 28 00 mov %fs:0x28,%rax115f: 00 001161: 48 89 45 f8 mov %rax,-0x8(%rbp)1165: 31 c0 xor %eax,%eaxstruct possibleSet p[10];return &amp;amp;p[0];1167: b8 00 00 00 00 mov $0x0,%eax}116c: 48 8b 55 f8 mov -0x8(%rbp),%rdx1170: 64 48 33 14 25 28 00 xor %fs:0x28,%rdx1177: 00 001179: 74 05 je 1180 &amp;lt;func+0x37&amp;gt;117b: e8 d0 fe ff ff callq 1050 &amp;lt;__stack_chk_fail@plt&amp;gt;1180: c9 leaveq1181: c3 retq 这里可以看到1167地址处mov $0x0,%eax，返回值（eax寄存器）被赋了零。如果对例子稍作修改，将局部变量修改为静态变量，那么反汇编里就没有类似的处理了（具体实验代码略）。</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://sukihiro.cn/about/</link>
      <pubDate>Sat, 23 Jan 2021 21:49:27 +0800</pubDate>
      
      <guid>https://sukihiro.cn/about/</guid>
      <description>80后
两个孩子的爹
喜欢钻研Kernel和底层技术，以及任何能介绍how it works的东西
吉他老男孩一枚</description>
    </item>
    
    <item>
      <title>操作系统对NUMA支持情况：OS部分</title>
      <link>https://sukihiro.cn/2020/03/25/numa-os-part2/</link>
      <pubDate>Wed, 25 Mar 2020 17:03:40 +0000</pubDate>
      
      <guid>https://sukihiro.cn/2020/03/25/numa-os-part2/</guid>
      <description>NUMA-Aware操作系统对NUMA系统的支持分为两个层面：
 操作系统本身完成大多数关于进程、内存、IO位置的决策，向用户程序隐藏NUMA拓扑信息的细节。这样做的好处操作系统可以进行全局的NUMA资源优化，同时用户程序可以透明的使用NUMA硬件资源而无需关心底层亲缘性细节。 操作系统同时提供一组用户接口（NUMA API），以供特殊需求的用户对NUMA内存分配、调度策略进行重载和显式的控制，以实现复杂应用的手动调优。  1. NUMA系统内存分配 Linux的NUMA系统的内存按照Node划分，一个NUMA Node对应一个Memory Node。一个Memory Node下根据不同的用途划分为不同的Zones（如ZONE_DMA、ZONE_NORMAL等）。
1.1 内存分配策略 NUMA系统下从哪里分配内存是由Memory Policies决定的。内存分配策略可以针对全系统级别的（System Default Policy），也可以针对某一进程（Task/Process Policy），还可以针对进程中的某一段区域（VMA Policy）。如果考虑到动态共享库问题，还包括共享级策略（Shared Policy）。就优先级而言，进程级的策略高于系统默认策略，而内存区域的策略又可以重载进程级的策略。系统的默认Policy是由系统进程级的内存分配策略可以通过fork进行继承。系统提供get/set_mempolicy和mbind系统调用，以供
内核支持的内存分配策略有三种：
 MPOL_BIND：绑定策略，只能从特定的一组Nodes上分配内存。 MPOL_PREFERED：即优先从指定的Node上分配，如果不能满足就再从其他Node上分配。如果指定的Node就是执行内存分配的CPU所在的Node，就称之为Local Allocation。 MPOL_INTERLEAVE：交替策略，它指定在指定的几个节点上，以页为单位，交叉分配内存（round-robin）。  系统的默认分配策略是本地分配（即Local Allocation），但是在启动过程中使用交替策略，这样内核的数据结构是分布在所有Node上的，从而避免启动核所在Node负载过重。一旦系统第一个进程（init）开始执行，策略就变成了本地分配。
1.2 first touch原则 设置内存分配策略并不触发任何实际的分配动作，没有访问过的虚拟空间或者ZERO Page事实上都没有对应实际的物理页面。因此，只有在第一次访问出发缺页时，这些内存分配策略才会其作用，称为First Touch原则。
一个进程可以设置自己的Memory Policy，但是如果多个进程共用同一物理页（如共享库情况），那么Memory Policy就有可能失效。因为内存页的第一个使用者将决定这个页的分配策略。
1.3 页面迁移 如果任务频繁使用的页面是在远程Node，就有必要将其迁移到本地Node。Linux提供给用户页面迁移的接口（migrate_pages/move_pages系统调用），其中migrate_pages可以把一个进程的所有页面迁移到制定Node，move_pages则是可以把一组页面迁移。普通权限用户只能迁移那些只被本进程引用的页面。要迁移所有页面需要ROOT权限。
但是很难保证将任务所有页面都迁移到本地节点，因为有些代码段是被大量的共享使用的，特别是考虑到动态链接库的情况下。
2. NUMA API 2.1 NUMA工具 numactl是设定进程NUMA策略的命令行工具。对于那些无法修改和重新编译的程序，它可以进行非常有效的策略设定。numactl使管理员可以通过简单的命令行调用来设定进程的策略， 并可以集成到管理脚本中。
numactl的主要功能包括：
 设定进程的内存分配基本策略 限定内存分配范围，如某一特定节点或部分节点集合 对进程进行节点或节点集合的绑定 修改命名共享内存，tmpfs或hugetblfs等的内存策略 获取当前策略信息及状态 获取NUMA硬件拓扑  下面是使用numactl设定进程策略的实例：
numactl --cpubind=0 --membind=0,1 program 其意义为：在节点0上的CPU运行名为program的程序，并且只在节点0，1上分配内存。Cpubind的参数是节点编号，而不是cpu编号。在每个节点上有多个CPU的系统上，编号的定义顺序可能会不同。
下面是使用numactl更改共享内存段的分配策略的实例：
numactl --length=1G --file=/dev/shm/interleaved --interleave=all 其意义为： 对命名共享内存interleaved进行设置，其策略为全节点交织分配，大小为1G。</description>
    </item>
    
    <item>
      <title>操作系统对NUMA支持情况：NUMA基础</title>
      <link>https://sukihiro.cn/2020/03/25/numa-os-part1/</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://sukihiro.cn/2020/03/25/numa-os-part1/</guid>
      <description>1. NUMA简介 NUMA表示Non-Uniform Memory Access，非均匀内存访问模型，通常是指内存到每个CPU的&amp;quot;距离&amp;quot;并不相同，这里&amp;quot;距离&amp;quot;同时包括延迟和带宽。在NUMA系统中，这种非均匀性还会扩展到IO资源（主要是PCIE设备），即CPU与IO资源的&amp;quot;距离&amp;quot;以及设备与内存的&amp;quot;距离&amp;quot;也是不同的。一般来说NUMA机器可以访问系统的所有资源（即统一编址特性），只是访问速率不同而已。
NUMA中使用Node表示一组资源（例如CPUs、Memory、I/O)。在某些系统中一个Node可能只包含某几类资源而不是全部，有些系统中一个Node就包括所以这些种类。Node间的互联有不同的形式，但是一般来说Node之间的延迟会比Node内大，而带宽通常会比Node内低。
NUMA上的软件聚焦在Locality上，即尽可能地使用离设备（这里CPU也是算设备）近的资源，减少Node间的数据流量交互。对于Node间有高速互联的系统，把对资源（内存和IO）的使用分散到不同的节点上，而不是仅仅使用本地资源，有时反而能够取得更好的系统带宽（原因是能够存储器控制器的利用率更高），但是这种方法并不普适。
2. NUMA Hierarchy NUMA Hierarchy就是NUMA的层级结构。一个Intel x86 NUMA系统就是由多个NUMA Node组成。
2.1 NUMA Node内部 一个NUMA Node内部是由一个物理CPU和它所有的本地内存(Local Memory)组成的。广义得讲， 一个NUMA Node内部还包含本地IO资源，对大多数Intel x86 NUMA平台来说，主要是PCIe总线资源。 ACPI规范就是这么抽象一个NUMA Node的。
物理CPU 一个CPU Socket里可以由多个CPU Core和一个Uncore部分组成。每个CPU Core内部又可以由两个CPU Thread组成。每个CPU thread都是一个操作系统可见的逻辑CPU。对大多数操作系统来说，一个八核HT打开的CPU会被识别为16个CPU。
  Socket
一个Socket对应一个物理CPU。这个词大概是从CPU在主板上的物理连接方式上来的。处理器通过主板的Socket来插到主板上。尤其是有了多核(Multi-core)系统以后，Multi-socket系统被用来指明系统到底存在多少个物理CPU。
  Core
CPU的运算核心。 x86的核包含了CPU运算的基本部件，如逻辑运算单元(ALU), 浮点运算单元(FPU), L1和L2缓存。一个Socket里可以有多个Core。如今的多核时代，即使是Single Socket的系统，也是逻辑上的SMP系统。但是，一个物理CPU的系统不存在非本地内存，因此相当于UMA系统。
  Uncore
Intel x86物理CPU里没有放在Core里的部件都被叫做Uncore。Uncore里集成了过去x86 UMA架构时代北桥芯片的基本功能。在Nehalem时代，内存控制器被集成到CPU里，叫做iMC(Integrated Memory Controller)。而PCIe Root Complex还做为独立部件在IO Hub芯片里。到了SandyBridge时代，PCIe Root Complex也被集成到了CPU里。现今的Uncore部分，除了iMC，PCIe Root Complex，还有QPI(QuickPath Interconnect)控制器，L3缓存，CBox(负责缓存一致性)，及其它外设控制器。
  Threads
这里特指CPU的多线程技术。在Intel x86架构下，CPU的多线程技术被称作超线程(Hyper-Threading)技术。Intel的超线程技术在一个处理器Core内部引入了额外的硬件设计模拟了两个逻辑处理器(Logical Processor)，每个逻辑处理器都有独立的处理器状态，但共享Core内部的计算资源，如ALU，FPU，L1，L2缓存。这样在最小的硬件投入下提高了CPU在多线程软件工作负载下的性能，提高了硬件使用效率。x86的超线程技术出现早于NUMA架构。
  本地内存 在Intel x86平台上，所谓本地内存，就是CPU可以经过Uncore部件里的iMC访问到的内存。而那些非本地的，远程内存(Remote Memory)，则需要经过QPI的链路到该内存所在的本地CPU的iMC来访问。曾经在Intel IvyBridge的NUMA平台上做的内存访问性能测试显示，远程内存访问的延时时本地内存的一倍。可以假设，操作系统应该尽量利用本地内存的低访问延迟特性来优化应用和系统的性能。</description>
    </item>
    
    <item>
      <title>RISC-V与OpenRISC</title>
      <link>https://sukihiro.cn/2018/04/24/riscv-vs-openrisc/</link>
      <pubDate>Tue, 24 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://sukihiro.cn/2018/04/24/riscv-vs-openrisc/</guid>
      <description>2016年的时候，第一次听说RISC-V，到去年在中国开RISC-V顶会，可以说RISC-V已经在商用体系架构领域站稳脚跟。作为一个开放架构，第一反应就是拿它和OpenRISC比较。研究了下，主要体现在许可证以及技术两个层面。
开源协议 OpenRISC和RISC-V都是开放处理器架构，但是二者的许可证不同。OpenRISC采用了GPL和LGPL作为其开源协议，其中硬件设计采用LGPL（宽松的GPL），而模型和固件采用GPL。了解开源License的都清楚，GPL的强传染性导致其很难商业化，因为基于GPL开发的东西也必须开源，对于二次开发的知识产权根本没法保护。宽松的GPL没有这么强的传染性，即以链接的方式适用LGPL库是不需要开源代码的。很显然，OpenRISC的模型和固件的强开源特性让很多商业公司无法参与到其社区及生态的建设当中。RISC-V则采用了BSD协议，这个协议的要求非常宽松，简单来说，对于BSD代码的使用，没有任何限制包括闭源和商用。很显然，RISC-V采用这个协议是为了吸引更多的人和公司参与到社区和生态的建设当中。当然，BSD协议本来就是Berkelay的东西（最早是用发布BSD Unix），从伯克利的文化风格来讲，RISC-V采用BSD也算是情理之中。
技术层面 从RISC-V的官方的这篇Blog来看，技术层面的分歧导致了它们没有基于OpenRISC开发，而是设计全新的ISA。OpenRISC的主要技术问题包括：
 立即数占了16位编码，浪费了太多的编码空间，导致很难对指令进行扩展。(注：RISCV的立即数最多占12位) 没有64位版本。尽管2010年已经开始了64位的相关工作，但是到2014年还没有可用的实现和软件栈。 OpenRISC架构里还有大量的分支延迟间隙（branch delay slot）。 没有压缩指令支持。  (完)</description>
    </item>
    
    <item>
      <title>IPMI小记</title>
      <link>https://sukihiro.cn/2017/11/23/ipmi-note/</link>
      <pubDate>Thu, 23 Nov 2017 01:40:05 +0000</pubDate>
      
      <guid>https://sukihiro.cn/2017/11/23/ipmi-note/</guid>
      <description>IPMI简介 智能平台管理接口（Intelligent Platform Management Interface）是一个开放的标准，定义了扩平台、跨操作系统监控服务器运行的接口。IPMI卡独立于服务器板主机系统，因此服务器的运行情况不影响监控功能，例如服务器宕机后，可以利用IPMI远程收集信息和重启。在计算机领域这种管理方式通常叫做带外管理**（**Out-of-band management），特指使用独立管理通道进行设备维护。它允许系统管理员远程监控和管理服务器和其他网络设备，无论这些设备是否处于开机状态。
IPMI只定义架构和接口格式，详细实现各厂商没有统一标准。
IPMI结构 IPMI的主控叫BMC（baseboard management controller），其他管理控制器叫做卫星控制器（satellite controllers）。BMC是定制的嵌入式控制器，运行管理系统软件；管理控制器与平台的硬件连接，获取温度、电源、风扇转速、操作系统状态等信心。BMC与周边控制器通过IPMB总线连接，IPMB是I2C的增强实现。BMC也可以通过IPMC总线或桥与其他的BMC及其附属控制器连接。
Side-band vs. out-of-band IPMI卡在实现上分两种，一种是BMC提供独立的NIC端口，对外提供远程服务，叫做out-of-band方式；另一种方式是BMC通过SMBus连接主机系统的NIC-Phy端口（如上图），与Host OS共享LAN接口，称作Side-band方式。Side-band的优点是可以节省成本较少线缆连接，缺点是带宽受限（因为是SMBus），一般只能支持文本控制台。另外，在Side-band方式下，一旦主机的LAN端口发生硬件故障，IPMI就没办法通过网络对外提供管理服务了。
关于Side-band的实现细节，即BMC和Host OS共享端口的技术细节，可以参考这篇文章和这个问答。其核心是NIC端口的多MAC支持功能，可以把端口的这种实现方式理解成一个网络交换机：将一个物理端口和两个虚拟端口桥接。问答里还列出了Side-band方式的更多缺陷，主要是BMC和HOST没有办法保证真正的隔离。</description>
    </item>
    
    <item>
      <title>Linux驱动模型Probe解惑</title>
      <link>https://sukihiro.cn/2017/10/31/linux-driver-model/</link>
      <pubDate>Tue, 31 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sukihiro.cn/2017/10/31/linux-driver-model/</guid>
      <description>问题 首先来回顾下，Linux设备驱动模型中bus、device和driver三者的关系：
 bus是物理总线的抽象。 device是设备抽象，存在于bus之上。 driver是驱动抽象，注册到bus上，用于驱动bus上的特定device。 device和driver通过bus提供的match方法来匹配（通常是使用设备ID进行匹配）。 driver匹配到device后，调用driver的probe接口驱动device。 一个driver可以驱动多个相同的设备或者不同的设备。  一个driver匹配并驱动多个device的情形比较常见，比如一个igb驱动可以驱动多块intel网卡（可以是相同型号，也可以是不同型号），这是由驱动的id_table和驱动的处理逻辑决定的。那么自然而然的一个问题是：如果系统有多个driver都可以匹配到一个device，系统会怎么处理？换句话说，Linux是否允许两个driver服务同一个device？
实验 带着这样的疑问，我做了下面这个实验。
首先，查看下系统的PCI设备驱动情况，挑选一个测试对象。
$ lspci 00:00.0 Host bridge: Intel Corporation Skylake Host Bridge/DRAM Registers (rev 08) 00:02.0 VGA compatible controller: Intel Corporation HD Graphics 520 (rev 07) 00:14.0 USB controller: Intel Corporation Sunrise Point-LP USB 3.0 xHCI Controller (rev 21) 00:14.2 Signal processing controller: Intel Corporation Sunrise Point-LP Thermal subsystem (rev 21) 00:16.0 Communication controller: Intel Corporation Sunrise Point-LP CSME HECI #1 (rev 21) 00:17.</description>
    </item>
    
    <item>
      <title>IRQ Balance</title>
      <link>https://sukihiro.cn/2017/02/05/irq-balance/</link>
      <pubDate>Sun, 05 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sukihiro.cn/2017/02/05/irq-balance/</guid>
      <description>关于Linux的中断的balance，这篇文章写的很好，可以学习一下。看完后我的理解是这样的:
 多核情况下，为了发挥并发优势，希望中断能分发到不同的核上并发处理，但是需要注意，数据在不同CPU的Cache上来回的迁移是很耗性能的，因此Linux的默认行为也是将某一中断绑定到一个核，而不是在多个核上轮播。 但是一般中断处理代码都比较短，Cache的数据量不大，因此如果能自动在多个CPU上轮播中断是有可能提升性能的。但是问题又来了，网卡的收发包过程中有TCP连接状态的缓存，是非常适合Cache进行缓存的，因此在这种情况下轮播中断变得不可取，因为会导致严重的Cache数据颠簸，性能大大下降。 为了解决这个问题，现代的网卡都是采用PCI MSIX中断，也就是单设备多中断，可以为每个硬件收发队列分配一个中断，然后把每个中断绑定到不同的核上，这样基本上就解决了上面的问题，即能多核并发，又能兼顾收发包的局部性缓存带来的性能收益（网卡驱动会根据IP地址的hash放到对应的队列。） 最后一点，服务器设备比较多，有可能发生高速设备的中断数大于CPU核数，那么就会有一个核处理2个以上中断的情况发上，而这个基本上是没办法提前预知的（和CPU架构、核数、设备拓扑等相关），所以只能通过/proc/irq接口来手动调节。有个用户程序叫irqbalance可以自动做中断均衡，而且据说新版本评价不错，我自己没试过，但我认为要想获取最佳性能还得手动设置affinity。  </description>
    </item>
    
    <item>
      <title>Docker初试</title>
      <link>https://sukihiro.cn/2017/01/02/try-docker/</link>
      <pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://sukihiro.cn/2017/01/02/try-docker/</guid>
      <description>要了解一样东西，最简单的入门方法无非是去使用它，跟它交互，一步步填补概念和纠正已有认知。从今天开始踏上Docker之路！
安装 ubuntu 16.04上的安装还是比较方便的，根据官方文档，添加APT源，然后直接apt-get install docker-engine就ok了。具体命令如下：
$ sudo apt-get update $ sudo apt-get install apt-transport-https ca-certificates $ sudo apt-key adv \ --keyserver hkp://ha.pool.sks-keyservers.net:80 \ --recv-keys 58118E89F3A912897C070ADBF76221572C52609D $ echo &amp;#34;deb https://apt.dockerproject.org/repo ubuntu-xenial main&amp;#34; | sudo tee /etc/apt/sources.list.d/docker.list $ sudo apt-get update $ sudo apt-get install linux-image-extra-$(uname -r) linux-image-extra-virtual $ sudo apt-get install docker-engine 小试 开启docker服务 docker daemon负责接收docker工具发过来的命令，然后做相应的操作，因此首先要开启它：
$ sudo service docker start hello world docker提供了一个最基本的hello world镜像，用来验证docker安装是否就绪。利用docker命令运行它：
$ sudo docker run hello-world 如果正常一切正常，应该有如下输出：</description>
    </item>
    
    <item>
      <title>readl和raw_readl语义区别</title>
      <link>https://sukihiro.cn/2016/08/27/raw_readl/</link>
      <pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://sukihiro.cn/2016/08/27/raw_readl/</guid>
      <description>进行Linux驱动方面开发的程序员，经常需要使用readl/writel系列函数对Memory-Mapped IO进行读写。那么readl/__readl/raw_readl/__raw_readl几个函数的具体语义有何差别呢？
内核中关于这几个函数的说明非常有限，根据stackoverflow上的这个问答，我们可以总结为：
  raw前缀只与byteorder相关，即readl/writel是linux默认的小端操作，而raw_readl/raw_writel是native访问。也就是说：如果是小端系统，raw_readl与readl相同，如果是大端系统，raw_readl与readl的有字节序差别。
  双下划线前缀与指令保序相关，即readl/writel包含存储器栅栏指令mb，能够保证IO读写顺序，而__readl/__writel则不能保证。
  另外，有的体系结构还会定义readl_relaxed/writel_relaxed接口，其含义应该与__readl/__writel相同，表示小端、不带存储器栅栏的读写。</description>
    </item>
    
    <item>
      <title>GNU三元操作符扩展</title>
      <link>https://sukihiro.cn/2016/03/03/ternary-op-extension/</link>
      <pubDate>Thu, 03 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://sukihiro.cn/2016/03/03/ternary-op-extension/</guid>
      <description>Linux内核随处可见到关于三元操作符？:的这种使用方法：
rdev-&amp;gt;map_name = map_name ?: RC_MAP_EMPTY; 这是GNU C的关于标准C三元操作符?:的一个扩展，用来省略条件表达式的中间参数。也就是说， x ? : y 与 x ？ x : y完全等价。
所有GNU扩展都在GCC手册的Extensions to the C Language Family章节有详细描述。要注意，这些写法仅适合GCC编译器，如果要编写可移植的代码，那么就要格外小心。</description>
    </item>
    
  </channel>
</rss>
